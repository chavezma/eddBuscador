{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructuras de Datos\n",
    "## Universidad Nacional de Tres de Febrero\n",
    "## Trabajo Práctico 2\n",
    "### Enunciado\n",
    "\n",
    "En este trabajo práctico vamos a desarrollar un buscador, para lo cual será necesario recorrer la web e indexar los documentos. Los módulos que tendrá nuestra aplicación serán los siguientes:\n",
    "- Módulo Crawler\n",
    "- Módulo Indice Invertido\n",
    "- Módulo Buscador\n",
    "\n",
    "\n",
    "### Módulo Crawler\n",
    "Este módulo será el encargado de recorrer la web. El archivo de configuración tendrá una sección especial ```[CRAWLER] ``` los parámetros de configuración son:\n",
    "\n",
    "```URLs:``` lista de URLs que determinan la frontera del Crawler, separadas por punto y coma \";\"\n",
    "\n",
    "```Log:``` Archivo donde se loggearan todas las páginas visitadas\n",
    "\n",
    "```Tmin:``` Tiempo mínimo que deberá respetar entre dos peticiones seguidas al mismo servidor\n",
    "\n",
    "El crawler deberá tomar cada una las URLs de su frontera, visitar la página, extraer los enlaces a otras páginas y si estas páginas se encuentran dentro de su frontera deberá visitarlas, si uno de estos enlaces apunta a una página fuera de su frontera deberá ignorar el enlace.\n",
    "El crawler deberá ser **Robusto** es decir no debe caer en bucles infinitos (por ejemplo si dos páginas o más páginas se referencian mutuamente) o si un servidor de aplicaciones genera páginas dinámicamente creando una trampa. La ejecución de este módulo se deberá poder interrumpir en cualquier momento con CTRL-C, y se deberá poder reiniciar en cualquier momento. Cuando se reinicia no debe empezar todo de nuevo, sino que deberá revisar el archivo de Salida para no visitar nuevamente las páginas que haya visitado anteriormente. También deberá ser **Cortez** es decir debe mantener solo una conexión abierta a un servidor dado, esperar un tiempo mínimo establecido por el parámetro Tmin entre dos peticiones seguidas al mismo servidor, y en cada ejecución del crawler visitar cada página solo una vez. Por ejemplo si varias páginas de un sitio contienen un enlace a una página de contacto, sólo podrá visitar la página de contacto una única vez.\n",
    "También deberá registrar en una bitácora todas las acciones que realice.\n",
    "Cuando el módulo crawler comienza a ejecutarse deberá agregar la siguiente línea al archivo de log:\n",
    "\n",
    "Crawler iniciado [dd/mm/aaaa hh:mm:ss] indicando la fecha y hora.\n",
    "\n",
    "Cuando el crawler termina de ejecutarse o se interrumpe su ejecución deberá registrar:\n",
    "\n",
    "Crawler finalizado [dd/mm/aaaa hh:mm:ss]\n",
    "\n",
    "y cada vez que visita una página (realiza una petición a un servidor) deberá registrar:\n",
    "\n",
    "url_visitada [dd/mm/aaaa/ hh:mm:ss]\n",
    "\n",
    "Si dentro de una URL inicial que contenía en su frontera no hay más enlaces por visitar deberá registrar la siguiente línea:\n",
    "\n",
    "url_inicial completa [dd/mm/aaaa/ hh:mm:ss]\n",
    "\n",
    "El archivo de log así definido debe permitir reconstruir el orden y el momento en que visitó todas y cada una de las páginas.\n",
    "\n",
    "En todos los casos el formato de fecha y hora deberá corresponder con la zona horaria de Argentina GMT-3\n",
    "\n",
    "El módulo crawler deberá extraer el texto de cada una de las páginas que visita y pasarlo al módulo de índice invertido para que lo registre. Para realizar esta operación se deberá utilizar expresiones regulares.\n",
    "\n",
    "A continuación se incluye una clase denominada ```LinkParser ``` que permite visitar una URL y devuelve la lista de URLs encontradas en esa dirección y el HTML crudo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse\n",
    "\n",
    "class LinkParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    newURL=parse.urljoin(self.baseURL, value)\n",
    "                    self.links.append(newURL)\n",
    "            \n",
    "    def fetch_page(self, url):\n",
    "        self.links=[]\n",
    "        self.baseURL = url\n",
    "        response = urlopen(url)\n",
    "        contentType=response.getheader('Content-type')\n",
    "        if 'text/html' in contentType:\n",
    "            encoding=response.headers.get_param('charset')\n",
    "            data=response.read()\n",
    "            htmlString=data.decode(encoding)\n",
    "            self.feed(htmlString)\n",
    "            return htmlString, self.links\n",
    "        else:\n",
    "            return \"\",[]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser=LinkParser()\n",
    "    resultados=parser.fetch_page(\"http://www.untref.edu.ar\")\n",
    "    print(\"página visitada: \\n\\n\", resultados[0])\n",
    "    print(\"\\n\\nEnlaces encontrados: \\n\\n\", resultados[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo Índice Invertido\n",
    "Este módulo será el responsable de mantener un índice invertido con todas las palabras encontradas en todas las páginas que visite el Crawler.\n",
    "\n",
    "La estructura de base deberá ser un ***árbol B con todos los datos en las hojas*** y deberá implementar las técnicas de compresión que veremos en clase.\n",
    "\n",
    "El índice deberá mantenerse en memoria y será utlizado por el módulo buscador para realizar consultas.\n",
    "\n",
    "El índice invertido se deberá construir ignorando STOP WORDS, palabras de longitud menor a ```min_long```, variable definida en el archivo de configuración en la sección ```[INVERTED_INDEX]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo Buscador\n",
    "Este módulo será la interfaz con el usuario, el cual podrá ingresar palabras a buscar y el sistema deberá devolver las páginas donde se encuentra esa palabra.\n",
    "Deberá soportar los siguientes comodines en la consulta:\n",
    "\n",
    "- \\*\n",
    "\n",
    "Las consulta consistirá en palabras separadas por espacios y el sistema deberá responder con las páginas que contengan todas las palabras que cumplen con la condición.\n",
    "Ejemplo:\n",
    "***\n",
    "\\*tref computación\n",
    "***\n",
    "Debe devolver las páginas que contengan palabras que terminan en tref y computación\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivo de configuración\n",
    "La aplicación deberá ser capaz de leer un archivo de configuración con el siguiente formato (se recomienda utilizar el módulo configparser).\n",
    "Deberá contener al menos las secciones y variables indicadas anteriormente y se le podrán agregar más variables y secciones\n",
    "Ejemplo, dado el siguiente archivo denominado \"config.ini\"\n",
    "\n",
    "```python\n",
    "[CRAWLER]\n",
    "URLs = http:\\\\www.untref.edu.ar ; http:\\\\www.uba.ar\n",
    "Log  = bitacora.log\n",
    "Tmin = 5\n",
    "\n",
    "[INVERTED_INDEX]\n",
    "min_long = 3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "configuracion=configparser.ConfigParser()\n",
    "\n",
    "configuracion.read(\"config.ini\")\n",
    "\n",
    "print(configuracion[\"CRAWLER\"][\"URLs\"])\n",
    "print(configuracion[\"CRAWLER\"][\"Log\"])\n",
    "print(configuracion[\"CRAWLER\"][\"Tmin\"])\n",
    "\n",
    "print(configuracion[\"INVERTED_INDEX\"][\"min_long\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
